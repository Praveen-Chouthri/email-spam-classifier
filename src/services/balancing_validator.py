"""
Balancing effectiveness validator for spam classification.

This module provides comprehensive validation of class balancing effectiveness,
including false negative rate validation, synthetic sample quality assessment,
and overall model performance validation.
"""
import logging
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report
from scipy import stats


@dataclass
class ValidationResults:
    """Results from balancing effectiveness validation."""
    fnr_target_met: bool
    fnr_original: float
    fnr_balanced: float
    fnr_improvement: float
    accuracy_maintained: bool
    accuracy_original: float
    accuracy_balanced: float
    accuracy_change: float
    synthetic_quality_score: float
    synthetic_quality_passed: bool
    overall_validation_passed: bool
    recommendations: List[str]
    detailed_metrics: Dict[str, Any]


class BalancingValidator:
    """
    Comprehensive validator for class balancing effectiveness.
    
    Validates that balancing achieves target false negative rate reduction,
    maintains synthetic sample quality, and improves overall performance.
    """
    
    def __init__(self, target_fnr: float = 0.05, min_accuracy_retention: float = 0.95):
        """
        Initialize the balancing validator.
        
        Args:
            target_fnr: Target false negative rate (default: 5%)
            min_accuracy_retention: Minimum accuracy retention ratio (default: 95%)
        """
        self.target_fnr = target_fnr
        self.min_accuracy_retention = min_accuracy_retention
        self.logger = logging.getLogger(__name__)
        
        # Validation thresholds
        self.synthetic_quality_threshold = 0.7
        self.min_improvement_threshold = 0.01  # 1% minimum improvement
        
        self.logger.info(f"Balancing validator initialized - Target FNR: {target_fnr:.1%}, "
                        f"Min accuracy retention: {min_accuracy_retention:.1%}")
    
    def validate_balancing_effectiveness(self, 
                                       original_results: Dict[str, Any],
                                       balanced_results: Dict[str, Any],
                                       synthetic_samples: Optional[np.ndarray] = None,
                                       original_samples: Optional[np.ndarray] = None) -> ValidationResults:
        """
        Comprehensive validation of balancing effectiveness.
        
        Args:
            original_results: Performance metrics from unbalanced model
            balanced_results: Performance metrics from balanced model
            synthetic_samples: Synthetic samples generated by SMOTE (optional)
            original_samples: Original training samples (optional)
            
        Returns:
            ValidationResults with comprehensive validation assessment
        """
        self.logger.info("Starting comprehensive balancing effectiveness validation...")
        
        # Validate false negative rate improvement
        fnr_validation = self._validate_fnr_improvement(original_results, balanced_results)
        
        # Validate accuracy retention
        accuracy_validation = self._validate_accuracy_retention(original_results, balanced_results)
        
        # Validate synthetic sample quality if available
        synthetic_quality_score = 0.8  # Default score if samples not provided
        synthetic_quality_passed = True
        
        if synthetic_samples is not None and original_samples is not None:
            synthetic_quality_score = self._validate_synthetic_quality(synthetic_samples, original_samples)
            synthetic_quality_passed = synthetic_quality_score >= self.synthetic_quality_threshold
        
        # Calculate detailed metrics
        detailed_metrics = self._calculate_detailed_validation_metrics(
            original_results, balanced_results, synthetic_quality_score
        )
        
        # Generate recommendations
        recommendations = self._generate_validation_recommendations(
            fnr_validation, accuracy_validation, synthetic_quality_score
        )
        
        # Determine overall validation result
        overall_passed = (
            fnr_validation['target_met'] and
            accuracy_validation['maintained'] and
            synthetic_quality_passed
        )
        
        results = ValidationResults(
            fnr_target_met=fnr_validation['target_met'],
            fnr_original=fnr_validation['original'],
            fnr_balanced=fnr_validation['balanced'],
            fnr_improvement=fnr_validation['improvement'],
            accuracy_maintained=accuracy_validation['maintained'],
            accuracy_original=accuracy_validation['original'],
            accuracy_balanced=accuracy_validation['balanced'],
            accuracy_change=accuracy_validation['change'],
            synthetic_quality_score=synthetic_quality_score,
            synthetic_quality_passed=synthetic_quality_passed,
            overall_validation_passed=overall_passed,
            recommendations=recommendations,
            detailed_metrics=detailed_metrics
        )
        
        self._log_validation_summary(results)
        return results
    
    def validate_model_performance_improvement(self, 
                                             y_true: np.ndarray,
                                             y_pred_original: np.ndarray,
                                             y_pred_balanced: np.ndarray) -> Dict[str, Any]:
        """
        Validate that balanced model shows improvement over original model.
        
        Args:
            y_true: True labels
            y_pred_original: Predictions from original (unbalanced) model
            y_pred_balanced: Predictions from balanced model
            
        Returns:
            Dictionary with performance improvement validation results
        """
        self.logger.info("Validating model performance improvement...")
        
        # Calculate metrics for both models
        original_metrics = self._calculate_performance_metrics(y_true, y_pred_original)
        balanced_metrics = self._calculate_performance_metrics(y_true, y_pred_balanced)
        
        # Calculate improvements
        improvements = {
            'accuracy': balanced_metrics['accuracy'] - original_metrics['accuracy'],
            'precision': balanced_metrics['precision'] - original_metrics['precision'],
            'recall': balanced_metrics['recall'] - original_metrics['recall'],
            'f1_score': balanced_metrics['f1_score'] - original_metrics['f1_score'],
            'fnr_reduction': original_metrics['fnr'] - balanced_metrics['fnr'],
            'fpr_change': balanced_metrics['fpr'] - original_metrics['fpr']
        }
        
        # Validate improvements
        validation_results = {
            'fnr_improved': improvements['fnr_reduction'] > self.min_improvement_threshold,
            'accuracy_maintained': improvements['accuracy'] >= -0.02,  # Allow 2% accuracy loss
            'recall_improved': improvements['recall'] > 0,
            'precision_acceptable': improvements['precision'] >= -0.05,  # Allow 5% precision loss
            'fpr_acceptable': improvements['fpr_change'] <= 0.05,  # Max 5% FPR increase
            'overall_improvement': False
        }
        
        # Overall improvement assessment
        validation_results['overall_improvement'] = (
            validation_results['fnr_improved'] and
            validation_results['accuracy_maintained'] and
            validation_results['fpr_acceptable']
        )
        
        return {
            'original_metrics': original_metrics,
            'balanced_metrics': balanced_metrics,
            'improvements': improvements,
            'validation': validation_results,
            'recommendation': self._get_improvement_recommendation(validation_results, improvements)
        }
    
    def validate_synthetic_sample_realism(self, 
                                        synthetic_samples: np.ndarray,
                                        original_samples: np.ndarray,
                                        feature_names: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Validate that synthetic samples maintain realistic spam characteristics.
        
        Args:
            synthetic_samples: Generated synthetic samples
            original_samples: Original training samples
            feature_names: Names of features (optional)
            
        Returns:
            Dictionary with synthetic sample quality assessment
        """
        self.logger.info(f"Validating realism of {len(synthetic_samples)} synthetic samples...")
        
        if len(synthetic_samples) == 0:
            return {'quality_score': 0.0, 'validation_passed': False, 'issues': ['No synthetic samples to validate']}
        
        validation_results = {
            'quality_score': 0.0,
            'validation_passed': False,
            'statistical_tests': {},
            'distribution_analysis': {},
            'outlier_analysis': {},
            'issues': [],
            'recommendations': []
        }
        
        try:
            # Statistical distribution comparison
            stat_results = self._compare_statistical_distributions(synthetic_samples, original_samples)
            validation_results['statistical_tests'] = stat_results
            
            # Feature distribution analysis
            dist_results = self._analyze_feature_distributions(synthetic_samples, original_samples)
            validation_results['distribution_analysis'] = dist_results
            
            # Outlier detection
            outlier_results = self._detect_synthetic_outliers(synthetic_samples, original_samples)
            validation_results['outlier_analysis'] = outlier_results
            
            # Calculate overall quality score
            quality_score = self._calculate_synthetic_quality_score(stat_results, dist_results, outlier_results)
            validation_results['quality_score'] = quality_score
            validation_results['validation_passed'] = quality_score >= self.synthetic_quality_threshold
            
            # Generate issues and recommendations
            if quality_score < self.synthetic_quality_threshold:
                validation_results['issues'] = self._identify_synthetic_quality_issues(
                    stat_results, dist_results, outlier_results
                )
                validation_results['recommendations'] = self._get_synthetic_quality_recommendations(
                    validation_results['issues']
                )
            
        except Exception as e:
            self.logger.error(f"Error during synthetic sample validation: {str(e)}")
            validation_results['issues'].append(f"Validation error: {str(e)}")
        
        return validation_results
    
    def _validate_fnr_improvement(self, original_results: Dict[str, Any], 
                                 balanced_results: Dict[str, Any]) -> Dict[str, Any]:
        """Validate false negative rate improvement."""
        # Extract FNR from results
        original_fnr = original_results.get('false_negative_rate', 
                                          original_results.get('metrics', {}).get('false_negative_rate', 0.0))
        balanced_fnr = balanced_results.get('false_negative_rate',
                                          balanced_results.get('metrics', {}).get('false_negative_rate', 0.0))
        
        improvement = original_fnr - balanced_fnr
        target_met = balanced_fnr <= self.target_fnr
        
        return {
            'original': original_fnr,
            'balanced': balanced_fnr,
            'improvement': improvement,
            'target_met': target_met,
            'target_value': self.target_fnr
        }
    
    def _validate_accuracy_retention(self, original_results: Dict[str, Any], 
                                   balanced_results: Dict[str, Any]) -> Dict[str, Any]:
        """Validate accuracy retention."""
        # Extract accuracy from results
        original_acc = original_results.get('accuracy', 
                                          original_results.get('metrics', {}).get('accuracy', 0.0))
        balanced_acc = balanced_results.get('accuracy',
                                          balanced_results.get('metrics', {}).get('accuracy', 0.0))
        
        change = balanced_acc - original_acc
        retention_ratio = balanced_acc / original_acc if original_acc > 0 else 0.0
        maintained = retention_ratio >= self.min_accuracy_retention
        
        return {
            'original': original_acc,
            'balanced': balanced_acc,
            'change': change,
            'retention_ratio': retention_ratio,
            'maintained': maintained,
            'min_retention': self.min_accuracy_retention
        }
    
    def _validate_synthetic_quality(self, synthetic_samples: np.ndarray, 
                                   original_samples: np.ndarray) -> float:
        """Validate quality of synthetic samples."""
        try:
            # Statistical similarity test
            similarity_scores = []
            
            for feature_idx in range(min(synthetic_samples.shape[1], original_samples.shape[1])):
                synthetic_feature = synthetic_samples[:, feature_idx]
                original_feature = original_samples[:, feature_idx]
                
                # Kolmogorov-Smirnov test for distribution similarity
                ks_stat, ks_p_value = stats.ks_2samp(synthetic_feature, original_feature)
                
                # Convert to similarity score (higher is better)
                similarity_score = 1.0 - min(ks_stat, 1.0)
                similarity_scores.append(similarity_score)
            
            # Average similarity across all features
            overall_quality = np.mean(similarity_scores) if similarity_scores else 0.0
            
            return float(overall_quality)
            
        except Exception as e:
            self.logger.error(f"Error validating synthetic quality: {str(e)}")
            return 0.5  # Default moderate score on error
    
    def _calculate_performance_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """Calculate comprehensive performance metrics."""
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
        
        # Calculate confusion matrix components
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        
        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'fnr': fnr,
            'fpr': fpr,
            'true_negatives': tn,
            'false_positives': fp,
            'false_negatives': fn,
            'true_positives': tp
        }
    
    def _compare_statistical_distributions(self, synthetic: np.ndarray, 
                                         original: np.ndarray) -> Dict[str, Any]:
        """Compare statistical distributions between synthetic and original samples."""
        results = {
            'ks_test_results': [],
            'mean_differences': [],
            'std_differences': [],
            'overall_similarity': 0.0
        }
        
        n_features = min(synthetic.shape[1], original.shape[1])
        
        for i in range(n_features):
            # Kolmogorov-Smirnov test
            ks_stat, ks_p = stats.ks_2samp(synthetic[:, i], original[:, i])
            results['ks_test_results'].append({'statistic': ks_stat, 'p_value': ks_p})
            
            # Mean and std differences
            mean_diff = abs(np.mean(synthetic[:, i]) - np.mean(original[:, i]))
            std_diff = abs(np.std(synthetic[:, i]) - np.std(original[:, i]))
            
            results['mean_differences'].append(mean_diff)
            results['std_differences'].append(std_diff)
        
        # Calculate overall similarity
        ks_similarities = [1.0 - min(test['statistic'], 1.0) for test in results['ks_test_results']]
        results['overall_similarity'] = np.mean(ks_similarities) if ks_similarities else 0.0
        
        return results
    
    def _analyze_feature_distributions(self, synthetic: np.ndarray, 
                                     original: np.ndarray) -> Dict[str, Any]:
        """Analyze feature distributions for quality assessment."""
        results = {
            'range_violations': 0,
            'extreme_outliers': 0,
            'distribution_shifts': 0,
            'quality_score': 0.0
        }
        
        n_features = min(synthetic.shape[1], original.shape[1])
        
        for i in range(n_features):
            orig_feature = original[:, i]
            synth_feature = synthetic[:, i]
            
            # Check range violations
            orig_min, orig_max = np.min(orig_feature), np.max(orig_feature)
            range_expansion = (orig_max - orig_min) * 0.2  # Allow 20% expansion
            
            if (np.min(synth_feature) < orig_min - range_expansion or 
                np.max(synth_feature) > orig_max + range_expansion):
                results['range_violations'] += 1
            
            # Check for extreme outliers
            orig_q75, orig_q25 = np.percentile(orig_feature, [75, 25])
            orig_iqr = orig_q75 - orig_q25
            outlier_threshold = orig_q75 + 3 * orig_iqr
            
            extreme_outliers = np.sum(synth_feature > outlier_threshold)
            if extreme_outliers > len(synth_feature) * 0.05:  # More than 5% outliers
                results['extreme_outliers'] += 1
        
        # Calculate quality score
        total_issues = results['range_violations'] + results['extreme_outliers']
        results['quality_score'] = max(0.0, 1.0 - (total_issues / n_features))
        
        return results
    
    def _detect_synthetic_outliers(self, synthetic: np.ndarray, 
                                  original: np.ndarray) -> Dict[str, Any]:
        """Detect outliers in synthetic samples."""
        from sklearn.ensemble import IsolationForest
        
        results = {
            'outlier_percentage': 0.0,
            'severe_outliers': 0,
            'quality_impact': 'low'
        }
        
        try:
            # Fit isolation forest on original samples
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            iso_forest.fit(original)
            
            # Predict outliers in synthetic samples
            outlier_predictions = iso_forest.predict(synthetic)
            outlier_count = np.sum(outlier_predictions == -1)
            
            results['outlier_percentage'] = (outlier_count / len(synthetic)) * 100
            results['severe_outliers'] = outlier_count
            
            # Assess quality impact
            if results['outlier_percentage'] > 20:
                results['quality_impact'] = 'high'
            elif results['outlier_percentage'] > 10:
                results['quality_impact'] = 'medium'
            else:
                results['quality_impact'] = 'low'
                
        except Exception as e:
            self.logger.error(f"Error in outlier detection: {str(e)}")
        
        return results
    
    def _calculate_synthetic_quality_score(self, stat_results: Dict, 
                                         dist_results: Dict, 
                                         outlier_results: Dict) -> float:
        """Calculate overall synthetic sample quality score."""
        # Weight different quality aspects
        stat_weight = 0.4
        dist_weight = 0.4
        outlier_weight = 0.2
        
        # Statistical similarity score
        stat_score = stat_results.get('overall_similarity', 0.0)
        
        # Distribution quality score
        dist_score = dist_results.get('quality_score', 0.0)
        
        # Outlier quality score (inverse of outlier percentage)
        outlier_pct = outlier_results.get('outlier_percentage', 0.0)
        outlier_score = max(0.0, 1.0 - (outlier_pct / 100))
        
        # Weighted average
        overall_score = (stat_weight * stat_score + 
                        dist_weight * dist_score + 
                        outlier_weight * outlier_score)
        
        return float(overall_score)
    
    def _calculate_detailed_validation_metrics(self, original_results: Dict, 
                                             balanced_results: Dict, 
                                             synthetic_quality: float) -> Dict[str, Any]:
        """Calculate detailed validation metrics."""
        return {
            'performance_comparison': {
                'original': original_results,
                'balanced': balanced_results
            },
            'synthetic_sample_quality': synthetic_quality,
            'validation_thresholds': {
                'target_fnr': self.target_fnr,
                'min_accuracy_retention': self.min_accuracy_retention,
                'synthetic_quality_threshold': self.synthetic_quality_threshold
            },
            'validation_timestamp': np.datetime64('now').astype(str)
        }
    
    def _generate_validation_recommendations(self, fnr_validation: Dict, 
                                           accuracy_validation: Dict, 
                                           synthetic_quality: float) -> List[str]:
        """Generate validation recommendations."""
        recommendations = []
        
        # FNR recommendations
        if not fnr_validation['target_met']:
            if fnr_validation['improvement'] > 0:
                recommendations.append(f"FNR improved by {fnr_validation['improvement']:.3f} but target not met. Consider adjusting target ratio or SMOTE parameters.")
            else:
                recommendations.append("FNR did not improve. Consider using class weights instead of SMOTE or adjusting balancing strategy.")
        else:
            recommendations.append(f"Excellent: FNR target achieved ({fnr_validation['balanced']:.3f} ≤ {self.target_fnr:.3f})")
        
        # Accuracy recommendations
        if not accuracy_validation['maintained']:
            recommendations.append(f"Accuracy retention below threshold ({accuracy_validation['retention_ratio']:.3f}). Consider reducing synthetic sample ratio.")
        else:
            recommendations.append("Good: Accuracy adequately maintained after balancing.")
        
        # Synthetic quality recommendations
        if synthetic_quality < self.synthetic_quality_threshold:
            recommendations.append(f"Synthetic sample quality below threshold ({synthetic_quality:.3f}). Consider adjusting k-neighbors or using different balancing method.")
        else:
            recommendations.append("Excellent: Synthetic samples maintain good quality characteristics.")
        
        return recommendations
    
    def _identify_synthetic_quality_issues(self, stat_results: Dict, 
                                         dist_results: Dict, 
                                         outlier_results: Dict) -> List[str]:
        """Identify specific synthetic sample quality issues."""
        issues = []
        
        if stat_results.get('overall_similarity', 1.0) < 0.6:
            issues.append("Statistical distributions significantly different from original samples")
        
        if dist_results.get('range_violations', 0) > 0:
            issues.append(f"Range violations detected in {dist_results['range_violations']} features")
        
        if outlier_results.get('outlier_percentage', 0) > 15:
            issues.append(f"High outlier percentage: {outlier_results['outlier_percentage']:.1f}%")
        
        return issues
    
    def _get_synthetic_quality_recommendations(self, issues: List[str]) -> List[str]:
        """Get recommendations for synthetic quality issues."""
        recommendations = []
        
        for issue in issues:
            if "statistical distributions" in issue.lower():
                recommendations.append("Increase k-neighbors parameter or reduce target ratio")
            elif "range violations" in issue.lower():
                recommendations.append("Add feature scaling or normalization before SMOTE")
            elif "outlier percentage" in issue.lower():
                recommendations.append("Use outlier detection preprocessing or reduce synthetic sample count")
        
        return recommendations
    
    def _get_improvement_recommendation(self, validation: Dict, improvements: Dict) -> str:
        """Get overall improvement recommendation."""
        if validation['overall_improvement']:
            return "Recommend using balanced model - significant improvement achieved"
        elif validation['fnr_improved'] and validation['accuracy_maintained']:
            return "Consider using balanced model - good FNR improvement with acceptable accuracy"
        elif not validation['accuracy_maintained']:
            return "Use original model - accuracy loss too significant"
        else:
            return "Evaluate based on business requirements - mixed results"
    
    def _log_validation_summary(self, results: ValidationResults) -> None:
        """Log comprehensive validation summary."""
        self.logger.info("=" * 60)
        self.logger.info("BALANCING EFFECTIVENESS VALIDATION SUMMARY")
        self.logger.info("=" * 60)
        self.logger.info(f"Overall Validation: {'PASSED' if results.overall_validation_passed else 'FAILED'}")
        self.logger.info(f"FNR Target Met: {'YES' if results.fnr_target_met else 'NO'} "
                        f"({results.fnr_balanced:.3f} vs target {self.target_fnr:.3f})")
        self.logger.info(f"FNR Improvement: {results.fnr_improvement:.3f} "
                        f"({results.fnr_original:.3f} → {results.fnr_balanced:.3f})")
        self.logger.info(f"Accuracy Maintained: {'YES' if results.accuracy_maintained else 'NO'} "
                        f"({results.accuracy_change:+.3f})")
        self.logger.info(f"Synthetic Quality: {results.synthetic_quality_score:.3f} "
                        f"({'PASS' if results.synthetic_quality_passed else 'FAIL'})")
        
        if results.recommendations:
            self.logger.info("Recommendations:")
            for i, rec in enumerate(results.recommendations, 1):
                self.logger.info(f"  {i}. {rec}")
        
        self.logger.info("=" * 60)